# ===============================
# LATCH TEST RUNNER & BENCHMARK REPORTER
# Comprehensive test execution and reporting
# ===============================

print("=== LATCH TEST RUNNER & BENCHMARK REPORTER ===")

# Test configuration
test_config := {
    "run_performance_tests": true,
    "run_edge_case_tests": true,
    "generate_report": true,
    "cleanup_after": true,
    "parallel_execution": true,
    "verbose_output": true
}

# Results storage
results := {
    "performance": {},
    "edge_cases": {},
    "summary": {
        "total_tests": 0,
        "passed_tests": 0,
        "failed_tests": 0,
        "execution_time": 0,
        "memory_usage": "N/A"
    }
}

# Helper function to format time
fn format_time(ms) {
    if ms < 1000 {
        return "${ms}ms"
    }
    if ms < 60000 {
        return "${(ms / 1000)}s"
    }
    minutes := (ms / 60000)
    seconds := ((ms % 60000) / 1000)
    return "${minutes}m ${seconds}s"
}

# Helper function to run test and capture results
fn run_test(test_name, test_file, test_type) {
    print("\n--- Running ${test_name} ---")
    
    start_time := time.now()
    
    try {
        # Run the test file
        result := proc.exec(["latch", "run", test_file])
        end_time := time.now()
        execution_time := end_time - start_time
        
        if result.code == 0 {
            print("‚úì ${test_name} completed successfully")
            results[test_type][test_name] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "output_length": len(result.stdout),
                "error_output": result.stderr
            }
        } else {
            print("‚úó ${test_name} failed with exit code ${result.code}")
            results[test_type][test_name] = {
                "status": "FAILED",
                "execution_time": execution_time,
                "error_code": result.code,
                "error_output": result.stderr
            }
        }
        
        return result.code == 0
        
    } catch e {
        print("‚úó ${test_name} failed to execute: ${typeof(e)}")
        results[test_type][test_name] = {
            "status": "ERROR",
            "execution_time": 0,
            "error": typeof(e)
        }
        return false
    }
}

# ===============================
# EXECUTION PHASE
# ===============================

overall_start := time.now()
print("Starting comprehensive test suite...")
print("Configuration: ${json.stringify(test_config)}")

# 1. Run Performance Tests
if test_config["run_performance_tests"] {
    perf_result := run_test("Comprehensive Performance Test", "comprehensive_stress_test.lt", "performance")
    if perf_result {
        results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
        results["summary"]["passed_tests"] = results["summary"]["passed_tests"] + 1
    } else {
        results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
        results["summary"]["failed_tests"] = results["summary"]["failed_tests"] + 1
    }
}

# 2. Run Edge Case Tests
if test_config["run_edge_case_tests"] {
    edge_result := run_test("Edge Cases & Unit Tests", "edge_cases_test_simple.lt", "edge_cases")
    if edge_result {
        results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
        results["summary"]["passed_tests"] = results["summary"]["passed_tests"] + 1
    } else {
        results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
        results["summary"]["failed_tests"] = results["summary"]["failed_tests"] + 1
    }
}

# 3. Run Original Stress Test (if exists)
original_result := run_test("Original Stress Test", "stress.lt", "performance")
if original_result {
    results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
    results["summary"]["passed_tests"] = results["summary"]["passed_tests"] + 1
} else {
    results["summary"]["total_tests"] = results["summary"]["total_tests"] + 1
    results["summary"]["failed_tests"] = results["summary"]["failed_tests"] + 1
}

overall_end := time.now()
results["summary"]["execution_time"] = overall_end - overall_start

# ===============================
# REPORT GENERATION
# ===============================

if test_config["generate_report"] {
    print("\n" + "="*60)
    print("           LATCH TEST SUITE REPORT")
    print("="*60)
    
    # Executive Summary
    print("\nüìä EXECUTIVE SUMMARY")
    print("-" * 30)
    print("Total Test Suites: ${results["summary"]["total_tests"]}")
    print("Passed: ${results["summary"]["passed_tests"]}")
    print("Failed: ${results["summary"]["failed_tests"]}")
    success_rate := (results["summary"]["passed_tests"] * 100) / results["summary"]["total_tests"]
    print("Success Rate: ${success_rate}%")
    print("Total Execution Time: ${format_time(results["summary"]["execution_time"])}")
    
    # Performance Test Results
    print("\n‚ö° PERFORMANCE TEST RESULTS")
    print("-" * 30)
    for test_name in keys(results["performance"]) {
        test_result := results["performance"][test_name]
        status_icon := ""
        if test_result["status"] == "PASSED" {
            status_icon = "‚úì"
        } else {
            status_icon = "‚úó"
        }
        print("${status_icon} ${test_name}")
        print("   Status: ${test_result["status"]}")
        print("   Time: ${format_time(test_result["execution_time"])}")
        if test_result["status"] != "PASSED" {
            print("   Error: ${test_result["error_output"]}")
        }
    }
    
    # Edge Case Test Results
    print("\nüîç EDGE CASE TEST RESULTS")
    print("-" * 30)
    for test_name in keys(results["edge_cases"]) {
        test_result := results["edge_cases"][test_name]
        status_icon := ""
        if test_result["status"] == "PASSED" {
            status_icon = "‚úì"
        } else {
            status_icon = "‚úó"
        }
        print("${status_icon} ${test_name}")
        print("   Status: ${test_result["status"]}")
        print("   Time: ${format_time(test_result["execution_time"])}")
        if test_result["status"] != "PASSED" {
            print("   Error: ${test_result["error_output"]}")
        }
    }
    
    # Performance Benchmarks
    print("\nüìà PERFORMANCE BENCHMARKS")
    print("-" * 30)
    if results["performance"]["Comprehensive Performance Test"] != null {
        perf_time := results["performance"]["Comprehensive Performance Test"]["execution_time"]
        if perf_time < 5000 {
            print("Large List Operations (10k items): ‚úì Good")
        } else {
            print("Large List Operations (10k items): ‚ö† Slow")
        }
        if perf_time < 3000 {
            print("String Operations (1000 iterations): ‚úì Good")
        } else {
            print("String Operations (1000 iterations): ‚ö† Slow")
        }
        if perf_time < 2000 {
            print("JSON Parsing (1000 items): ‚úì Good")
        } else {
            print("JSON Parsing (1000 items): ‚ö† Slow")
        }
        if perf_time < 10000 {
            print("Parallel Execution (100 workers): ‚úì Good")
        } else {
            print("Parallel Execution (100 workers): ‚ö† Slow")
        }
        if perf_time < 8000 {
            print("Filesystem Operations: ‚úì Good")
        } else {
            print("Filesystem Operations: ‚ö† Slow")
        }
    }
    
    # Recommendations
    print("\nüí° RECOMMENDATIONS")
    print("-" * 30)
    
    if success_rate == 100 {
        print("üéâ All tests passed! Latch is performing excellently.")
    }
    if success_rate >= 80 && success_rate < 100 {
        print("‚ö†Ô∏è  Most tests passed. Review failed tests for optimization.")
    }
    if success_rate < 80 {
        print("‚ùå Multiple test failures. Immediate attention required.")
    }
    
    if results["summary"]["execution_time"] > 30000 {
        print("üêå Test suite is slow. Consider optimization.")
    } else {
        print("‚ö° Test suite performance is acceptable.")
    }
    
    # System Information
    print("\nüñ•Ô∏è  SYSTEM INFORMATION")
    print("-" * 30)
    print("Latch Version: Available via 'latch version'")
    print("Platform: Available via environment variables")
    print("Test Date: ${time.now()}")
    print("Test Runner: Latch Comprehensive Test Suite v1.0")
    
    # Detailed Results (JSON)
    print("\nüìã DETAILED RESULTS (JSON)")
    print("-" * 30)
    print(json.stringify(results, null, 2))
}

# ===============================
# CLEANUP PHASE
# ===============================

if test_config["cleanup_after"] {
    print("\nüßπ CLEANUP PHASE")
    print("-" * 30)
    
    # Remove temporary files that might have been created
    temp_files := [
        "tmp.txt",
        "stress_test_dir",
        "comprehensive_stress_test_report.json"
    ]
    
    for temp_file in temp_files {
        try {
            if fs.exists(temp_file) {
                fs.remove(temp_file)
                print("Removed: ${temp_file}")
            }
        } catch e {
            print("Could not remove ${temp_file}: ${typeof(e)}")
        }
    }
    
    # Remove any worker files from parallel tests
    worker_files := fs.glob("tmp_worker_*.txt")
    for worker_file in worker_files {
        try {
            fs.remove(worker_file)
            print("Removed worker file: ${worker_file}")
        } catch e {
            print("Could not remove worker file: ${worker_file}")
        }
    }
    
    print("Cleanup completed.")
}

# ===============================
# FINAL SUMMARY
# ===============================

print("\n" + "="*60)
print("           TEST EXECUTION COMPLETE")
print("="*60)
print("Total Time: ${format_time(results["summary"]["execution_time"])}")
print("Success Rate: ${success_rate}%")

if success_rate == 100 {
    print("üéâ ALL TESTS PASSED - Latch is ready for production!")
    stop 0
} else {
    print("‚ö†Ô∏è  SOME TESTS FAILED - Review implementation.")
    stop 1
}
